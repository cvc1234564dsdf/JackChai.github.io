[{"title":"实验一","url":"/2024/10/18/实验-spark/","content":"from pyspark import SparkConf,SparkContext\n\nconf = SparkConf().setMaster(\"local\")\n\nsc = SparkContext(conf = conf)\n\nrdd1 = sc.textFile(\"result_bigdata.csv\").filter(lambda x:x!='ID,course,score').map(lambda x:x.split(\",\"))\n\nrdd1.collect()\n\nrdd1.take(5)\n\nmrdd1 = rdd1.map(lambda x:(int(x[0]),int(x[2])))\n\nmrdd1.collect()\n\nrdd2 = sc.textFile(\"result_math.csv\").filter(lambda x:x!='ID,course,score').map(lambda x:x.split(\",\"))\n\nrdd2.collect()\n\nmrdd2 = rdd1.map(lambda x:(int(x[0]),int(x[2])))\n\nmrdd2.collect()\n\nrdd2.take(5)\n\njrdd = mrdd1.join(mrdd2)\n\njrdd.collect()\n\nsjrdd = jrdd.sortBy(lambda x:x[0])\n\nsjrdd.collect()\n\nmrdd = jrdd.filter(lambda x: x[1][0]==100 or x[1][1]==100)\n\nmrdd.collect()\n\nsmrdd = jrdd.map(lambda x:(x[0],sum(x[1])))\n\nsmrdd.collect()\n\namrdd = jrdd.map(lambda x:(x[0],sum(x[1])/len(x[1])))\n\namrdd.collect()\n\nrddmax = jrdd.map(lambda x:(x[0],max(x[1])))\n\nrddmax.collect()\n\nrddmin = jrdd.map(lambda x:(x[0],min(x[1])))\n\nrddmin.collect()\n\ncrdd1 = rdd1.map(lambda x:(x[1],int(x[2])))\n\ncrdd1.collect()\n\ncrdd2 = rdd2.map(lambda x:(x[1],int(x[2])))\n\ncrdd2.collect()\n\nurdd = crdd1.union(crdd2)\n\nurdd.collect()\n\nssrdd = urdd.reduceByKey(lambda x,y : x+y)\n\nssrdd.collect()","categories":["spark"]}]